{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKb8rezOlnKw"
      },
      "source": [
        "## Deep Learning for Audio\n",
        "\n",
        "In this practical session, we will embark on an exciting journey into the world of deep learning, applied to the field of audio classification. Audio classification is a pivotal task in various applications, such as music genre identification, speech recognition, and environmental sound detection, to name a few. The essence of this task lies in the ability to accurately categorize audio clips into predefined classes based on their content.\n",
        "\n",
        "\n",
        "The session is structured to provide a comprehensive understanding of the process, including loading and visualizing audio files, extracting meaningful features like Mel-Frequency Cepstral Coefficients (MFCCs) and spectrograms, and preparing the dataset for training. We will then delve into building a convolutional neural network (CNN) model, which has proven effective in capturing the spatial hierarchy of features in audio data. Finally, students will apply their model to new data, seeing the culmination of their work in action.\n",
        "\n",
        "Additionally, in this practical, we will focus on a specific application of audio classification: speaker identification.\n",
        "\n",
        "You can find this dataset on this link: https://drive.google.com/file/d/1-6Soa4dM9nQi-hjoz021H2aEdhw-3MV9/view?usp=sharing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwfcIhleRLWw"
      },
      "outputs": [],
      "source": [
        "# For audio processing\n",
        "\n",
        "!pip install torchaudio\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BhNUswRn8uz"
      },
      "source": [
        "If you work on Colab:\n",
        "\n",
        "- Download the dataset using gdown (or upload the dataset on your drive)\n",
        "- Execute the following cell (replace with the path of your drive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "def download_public_file():\n",
        "    \"\"\"Downloads a public blob from the bucket.\"\"\"\n",
        "    bucket_name = \"data_ecoleit\"\n",
        "    source_blob_name = \"Copie de train-dataset.tar.gz\"\n",
        "    destination_file_name = \"./train-dataset.tar.gz\"\n",
        "\n",
        "    storage_client = storage.Client.create_anonymous_client()\n",
        "\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(source_blob_name)\n",
        "    blob.download_to_filename(destination_file_name)\n",
        "\n",
        "    print(\n",
        "        \"Downloaded public blob {} from bucket {} to {}.\".format(\n",
        "            source_blob_name, bucket.name, destination_file_name\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "download_public_file()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip4SRBH51F8I",
        "outputId": "fa4b43c9-b635-4b0e-b77a-686789e89294"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-6Soa4dM9nQi-hjoz021H2aEdhw-3MV9\n",
            "From (redirected): https://drive.google.com/uc?id=1-6Soa4dM9nQi-hjoz021H2aEdhw-3MV9&confirm=t&uuid=1db2f3c6-44ad-4188-92a5-a977a99bebd6\n",
            "To: /content/train-dataset.tar.gz\n",
            "100% 5.48G/5.48G [01:51<00:00, 49.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/uc?id=1-6Soa4dM9nQi-hjoz021H2aEdhw-3MV9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjMISMv_a84k"
      },
      "outputs": [],
      "source": [
        "# untar\n",
        "!tar xvf train-dataset.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbHLkuzZsWbq"
      },
      "source": [
        "#### Some explanations\n",
        "\n",
        "When you load an audio file, what you're essentially doing is converting an analog audio signal into a digital format that can be processed by a computer. This digital representation is a series of discrete numerical values that correspond to the audio signal's amplitude over time.\n",
        "\n",
        "**Waveform**: The plots you will generate show the waveform of the audio file. A waveform visually represents the variations in amplitude of the audio signal over time. Peaks in the waveform indicate high amplitude sounds (louder parts), while troughs represent low amplitude sounds (quieter parts).\n",
        "\n",
        "**Sample Rate**: This indicates the number of samples of audio carried per second and is measured in Hz. A higher sample rate means the audio is sampled more frequently and can capture more detail, but it also requires more data. LibROSA standardizes this to 22.05 kHz for consistency and processing efficiency, while TorchAudio retains the original sample rate, which can provide a more accurate representation of the original audio at the cost of increased complexity.\n",
        "\n",
        "**Duration**: The length of the audio clip in seconds. It's derived from the total number of samples divided by the sample rate. Understanding the duration is crucial for processing and analyzing the audio in segments or ensuring it fits within the required timeframe for your application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHOfHiWWqsZA"
      },
      "source": [
        "### Loading Audio with LibROSA\n",
        "\n",
        "- Install LibROSA if you haven't already: pip install librosa.\n",
        "\n",
        "- Use the librosa.load() function to load an audio file. Note that LibROSA automatically resamples the audio to 22.05 kHz by default.\n",
        "\n",
        "Resampling an audio signal refers to the process of changing the sample rate of the audio data. The sample rate is the number of samples (individual audio data points) captured per second, measured in Hertz (Hz). When you resample audio, you are essentially altering the number of these data points per second in the digital representation of the sound.\n",
        "\n",
        "\n",
        "\n",
        "- Display the sample rate and the duration of the audio.\n",
        "\n",
        "- Display the waveform of the audio in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4a11cYCq2Nj"
      },
      "source": [
        "### Loading Audio with TorchAudio\n",
        "\n",
        "- Use torchaudio.load() to load the same audio file. Unlike LibROSA, TorchAudio will retain the original sampling rate of the audio file.\n",
        "\n",
        "- Display the sample rate and the duration of the audio\n",
        "\n",
        "- Display the waveform of the audio in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T9FEkIlr1bm"
      },
      "source": [
        "**Analog Audio Signal**:\n",
        "\n",
        "An analog audio signal is a continuous representation of sound waves. Unlike digital audio, which represents sound with discrete numerical values at specific intervals (samples), an analog signal is continuous, with its amplitude varying smoothly over time. This type of signal closely mimics the physical sound waves that travel through the air and are captured by our ears.\n",
        "\n",
        "In the context of audio recording:\n",
        "\n",
        "**Recording**: Microphones convert sound waves (pressure changes in the air caused by vibrations) into analog electrical signals. These signals vary in voltage in a way that directly corresponds to the variation in air pressure of the sound waves.\n",
        "\n",
        "The transition from analog to digital (and vice versa) is a crucial part of modern audio processing. To digitize an analog signal for use in computers and digital devices, the continuous signal must be sampled at regular intervals, a process that involves measuring the amplitude of the signal at these points and encoding these measurements as digital data. This digital representation allows for extensive processing, editing, and transmission in ways that analog signals, constrained by physical media, cannot easily support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53ItXz4UvI8F"
      },
      "source": [
        "### Time domain vs Frequency domain\n",
        "\n",
        "What is the Time Domain?\n",
        "\n",
        "The time domain representation of an audio signal shows how the signal's amplitude varies over time. It's the most intuitive way to understand sound, as it directly represents what happens in the real world: sounds get louder and quieter over time, which corresponds to the peaks and troughs in the waveform.\n",
        "\n",
        "\n",
        "What is the Frequency Domain?\n",
        "\n",
        "Unlike the time domain, the frequency domain represents the audio signal in terms of its constituent frequencies. It shows how much of each frequency is present in the signal, rather than how the signal changes over time. This is crucial for understanding the pitch, tone, and harmonics of sounds.\n",
        "\n",
        "\n",
        "Fourier Transform:\n",
        "\n",
        "The Fourier Transform is a mathematical technique that transforms a signal from the time domain to the frequency domain. The Fast Fourier Transform (FFT) is a computationally efficient version of the Fourier Transform and is widely used in digital signal processing.\n",
        "\n",
        "Extracting and Visualizing Frequencies:\n",
        "\n",
        "You can use numpy or scipy to perform an FFT on your audio signal and extract its frequency components.\n",
        "Below is an example code snippet that performs FFT on an audio signal and visualizes its frequency domain representation\n",
        "\n",
        "You can also use librosa (check the documentation: https://librosa.org/doc/0.10.1/generated/librosa.stft.html#librosa-stft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-bkJrPJ3XBE"
      },
      "source": [
        "So now load an audio with librosa for instance and compute the Short time Fourier Transform and display the frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-R-Q5WE8rUM"
      },
      "source": [
        "Now load and play some audio in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwrzoWIN8Ea6"
      },
      "source": [
        "### Let's build a model now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JORSabLIA5iw"
      },
      "source": [
        "#### Implement the train / test split function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf6a-8a08L4O"
      },
      "source": [
        "First you need to implement a function that is going to select some image path for the training and some other for the validation.\n",
        "\n",
        "If you check the folder architecture, you can see we have multiple folders inside the `train-dataset` folder. Each folder is for a specific speaker.\n",
        "\n",
        "We want to have some audio samples for each speaker in both train and test dataset.\n",
        "\n",
        "The simplest way to achieve this to extract the audio path for both training and test. And then load dynamically the audio during the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_test_split():\n",
        "    speaker_folders = glob.glob(os.path.join(\"train-dataset\", '*'))\n",
        "\n",
        "    train_files, val_files = [], []\n",
        "    for speaker_folder in speaker_folders:\n",
        "        speaker_files = glob.glob(os.path.join(speaker_folder, \"*.flac\"))\n",
        "\n",
        "        nb_files = len(speaker_files)\n",
        "        nb_sample_val = int(nb_files * 0.3)\n",
        "        speaker_files_random = np.random.permutation(speaker_files)\n",
        "\n",
        "        val_path = speaker_files_random[:nb_sample_val]\n",
        "        train_path = speaker_files_random[nb_sample_val:]\n",
        "\n",
        "        train_files.extend(train_path.tolist())\n",
        "        val_files.extend(val_path.tolist())\n",
        "\n",
        "    return train_files, val_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_files, val_files = train_test_split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUn4K3wVA-Ko"
      },
      "source": [
        "#### Implement the torch dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWXJY5r8_fVJ"
      },
      "source": [
        "There are multiple pathways to architect your model. Specifically, your model can either operate directly on Mel Frequency Cepstral Coefficients (MFCCs) or utilize the spectrogram as its input. Each approach offers unique insights into the audio data, potentially influencing the model's performance based on the task at hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ay_QdB6Bd6K"
      },
      "source": [
        "You need to manage 3 types of features:\n",
        "\n",
        "- mfcc -> shape: (256, time duration)\n",
        "- mfcc with mean pooling on the time axis -> shape: (256,)\n",
        "- spectogram -> shape: (201, time duration)\n",
        "- spectogram with mean pooling on the time axis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faIDWg04AB5W"
      },
      "source": [
        "Implement the torch dataset that is going to use the mfcc or the spectogram. You can start by the mfcc dataset implementation first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdsQ7KW9AWlz"
      },
      "source": [
        "IMPORTANT: To simplify the task and account for varying audio lengths, you are encouraged to randomly select a 3-second segment from each audio file for processing. This approach ensures consistency in data input size and potentially streamlines the learning model's training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSrIfyETBP0p"
      },
      "source": [
        "Also do not forget that your dataset must return the feature vector (mfcc for instance) and the label (which is the id / name of the folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchaudio\n",
        "\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, file_paths):\n",
        "        self.file_paths = file_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_paths[idx]\n",
        "        waveform, sr = torchaudio.load(file_path)\n",
        "        waveform = waveform.squeeze()\n",
        "\n",
        "        # Extract 3 seconds of our audio\n",
        "        # case: audio < 3 sec\n",
        "        if int(len(waveform) - 3 * sr) <= 0:\n",
        "            zeros_tensor = torch.zeros(3 * sr - len(waveform))\n",
        "            w_sample = torch.cat([waveform, zeros_tensor])\n",
        "            print(len(waveform) / sr)\n",
        "        # case: audio > 3 sec\n",
        "        else: \n",
        "            rd_idx = torch.randint(0, int(len(waveform) - 3 * sr), (1,)).item()\n",
        "            w_sample = waveform[rd_idx:rd_idx + 3 * sr]\n",
        "\n",
        "        id_label = int(file_path.split('/')[1])\n",
        "\n",
        "        spec_features_extraction = torchaudio.transforms.Spectrogram()\n",
        "        features = spec_features_extraction(w_sample)\n",
        "\n",
        "        return features, id_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = AudioDataset(train_files)\n",
        "val_dataset = AudioDataset(val_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip7ADFXgBMIE"
      },
      "source": [
        "Now display the shape of one sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3lR8zzzD-ap"
      },
      "source": [
        "Build the DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r21febraEFzT"
      },
      "source": [
        "Now display the mfcc and the spectogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjXMbRE0FQ1Z"
      },
      "source": [
        "#### Build the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOhnVIbeFb3K"
      },
      "source": [
        "In this part you need to build multiple models:\n",
        "\n",
        "- Simple dense classifier for the pooling feature extraction versions\n",
        "\n",
        "- 1D Convolution model\n",
        "\n",
        "- 2D Convolution model\n",
        "\n",
        "For each version, play with the architecture, try to add Dropout, BatchNorm2D, residual connexions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzBimJR9GXLN"
      },
      "source": [
        "Implement a simple function that is going to count the number of learning parameters of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hLyJAlsGceW"
      },
      "source": [
        "#### Let's train the model\n",
        "\n",
        "Now you know, play with the optimizer, the loss ...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now display some predictions with the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "714TnIGrKv3h"
      },
      "source": [
        "Now compare the results of all the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NglArlGZKx4H"
      },
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVHPwpZrKzv9"
      },
      "source": [
        "Have a look at other metrics, for instance, how to compute precision / recall and f1 score in the case of multiclass classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mx7htubDK6P-"
      },
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNuOba0MZC9J"
      },
      "source": [
        "#### Now build an embedding model and visualize embeddings\n",
        "\n",
        "Follow the steps:\n",
        "\n",
        "- Create a new model with the same architecture as your best model except it must output the features instead of the class.\n",
        "\n",
        "- Use the pretrained weights of your best model\n",
        "\n",
        "- Focus on 20 class (we have more than 200 its too much to visualize something)\n",
        "\n",
        "- Apply the PCA with 3 components\n",
        "\n",
        "- Then make a 3D scatter plot with plotly to plot the 3D embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FIXME"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
